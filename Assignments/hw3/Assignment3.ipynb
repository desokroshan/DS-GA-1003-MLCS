{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling the file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import pdb\n",
    "from collections import Counter\n",
    "'''\n",
    "Note:  This code is just a hint for people who are not familiar with text processing in python. There is no obligation to use this code, though you may if you like. \n",
    "'''\n",
    "\n",
    "\n",
    "def folder_list(path,label):\n",
    "    '''\n",
    "    PARAMETER PATH IS THE PATH OF YOUR LOCAL FOLDER\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    review = []\n",
    "    for infile in filelist:\n",
    "        file = os.path.join(path,infile)\n",
    "        r = read_data(file)\n",
    "        r.append(label)\n",
    "        review.append(r)\n",
    "    return review\n",
    "\n",
    "def read_data(file):\n",
    "    '''\n",
    "    Read each file into a list of strings. \n",
    "    Example:\n",
    "    [\"it's\", 'a', 'curious', 'thing', \"i've\", 'found', 'that', 'when', 'willis', 'is', 'not', 'called', 'on', \n",
    "    ...'to', 'carry', 'the', 'whole', 'movie', \"he's\", 'much', 'better', 'and', 'so', 'is', 'the', 'movie']\n",
    "    '''\n",
    "    f = open(file)\n",
    "    lines = f.read().split(' ')\n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "    words = list(map(lambda Element: Element.translate(str.maketrans({char: None for char in symbols})).strip(), lines))\n",
    "    words = list(filter(lambda x:x!='', words))\n",
    "    return words\n",
    "\t\n",
    "###############################################\n",
    "######## YOUR CODE STARTS FROM HERE. ##########\n",
    "###############################################\n",
    "\n",
    "def shuffle_data():\n",
    "    '''\n",
    "    pos_path is where you save positive review data.\n",
    "    neg_path is where you save negative review data.\n",
    "    '''\n",
    "    pos_path = 'C:\\\\Users\\\\kumar\\\\Downloads\\\\MLCS\\\\Assignments\\\\A3\\\\hw3\\\\hw3-sentiment\\\\data\\\\data\\\\neg'\n",
    "    neg_path = 'C:\\\\Users\\\\kumar\\\\Downloads\\\\MLCS\\\\Assignments\\\\A3\\\\hw3\\\\hw3-sentiment\\\\data\\\\data\\\\pos'\n",
    "\t\n",
    "    pos_review = folder_list(pos_path,1)\n",
    "    neg_review = folder_list(neg_path,-1)\n",
    "\t\n",
    "    review = pos_review + neg_review\n",
    "    random.shuffle(review)\n",
    "    return review\n",
    "\t\n",
    "    \n",
    "def bag_of_words(reviews):\n",
    "    '''\n",
    "    Accepts a review (a list of words) and convert it into a bag of words\n",
    "    Return a list of tuples containing bag of words and correxponding result\n",
    "    '''\n",
    "    return [(Counter(review[:-1]), review[-1]) for review in reviews]\n",
    "\n",
    "'''\n",
    "Now you have read all the files into list 'review' and it has been shuffled.\n",
    "Save your shuffled result by pickle.\n",
    "*Pickle is a useful module to serialize a python object structure. \n",
    "*Check it out. https://wiki.python.org/moin/UsingPickle\n",
    "'''\n",
    "print(\"Pickling the file\")\n",
    "pickle.dump(shuffle_data(), open(\"review.p\",\"wb\")) \n",
    "reviews = pickle.load( open( \"review.p\", \"rb\" ) )\n",
    "train_data = reviews[:1500]\n",
    "validation_data = reviews[1500:2000]\n",
    "train_data = bag_of_words(train_data)\n",
    "validation_dataset = bag_of_words(validation_data)\n",
    "#print(train_data[1])\n",
    "# Split into 1500 training and 500 validation examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dotProduct(d1, d2):\n",
    "    \"\"\"\n",
    "    @param dict d1: a feature vector represented by a mapping from a feature (string) to a weight (float).\n",
    "    @param dict d2: same as d1\n",
    "    @return float: the dot product between d1 and d2\n",
    "    \"\"\"\n",
    "    if len(d1) < len(d2):\n",
    "        return dotProduct(d2, d1)\n",
    "    else:\n",
    "        return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
    "\n",
    "def increment(d1, scale, d2):\n",
    "    \"\"\"\n",
    "    Implements d1 += scale * d2 for sparse vectors.\n",
    "    @param dict d1: the feature vector which is mutated.\n",
    "    @param float scale\n",
    "    @param dict d2: a feature vector.\n",
    "\n",
    "    NOTE: This function does not return anything, but rather\n",
    "    increments d1 in place. We do this because it is much faster to\n",
    "    change elements of d1 in place than to build a new dictionary and\n",
    "    return it.\n",
    "    \"\"\"\n",
    "    for f, v in d2.items():\n",
    "        d1[f] = d1.get(f, 0) + v * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine via Pegasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for Normal Pegasos\n",
      "Running  0\n",
      "15.494833277795522\n",
      "bad : 1.1459027315123254\n",
      "movie : 0.9793471019320459\n",
      "only : 0.7994670219853444\n",
      "have : 0.6928714190539653\n",
      "? : 0.6862091938707519\n",
      "no : 0.6795469686875413\n",
      "at : 0.6195869420386428\n",
      "they : 0.6062624916722185\n",
      "even : 0.592938041305796\n",
      "could : 0.5263157894736848\n",
      "to : 0.4930046635576274\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Normal Pagasos Implementation\n",
    "from timeit import default_timer\n",
    "def pegasos(Lambda, data, max_iters):\n",
    "    '''\n",
    "    Args\n",
    "        Lambda - Regularization parameter\n",
    "    Returns:\n",
    "        w - a asparse weight vector w\n",
    "    '''\n",
    "    iters = 0\n",
    "    t=1\n",
    "    w=Counter()\n",
    "    # a simple termination condition for now\n",
    "    while(iters<max_iters):\n",
    "        print(\"Running \", iters)\n",
    "        iters = iters+1\n",
    "        for data in train_data:\n",
    "            t=t+1\n",
    "            eta = 1.0/(t*Lambda)\n",
    "            y = data[1]\n",
    "            x = data[0]\n",
    "            if (y*dotProduct(w,x)<1):\n",
    "                increment(w, -1.0*(Lambda*eta), w)\n",
    "                increment(w, eta*y ,x)\n",
    "            else:\n",
    "                increment(w, -1.0*(Lambda*eta), w)\n",
    "    return w\n",
    "print(\"Time for Normal Pegasos\")\n",
    "start = default_timer()\n",
    "w_opt1 = pegasos(0.1, train_data, 1)\n",
    "print(default_timer()-start)\n",
    "i =0\n",
    "for k,v in w_opt1.most_common():\n",
    "    print(k,\":\", v)\n",
    "    i=i+1\n",
    "    if (i>10): break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for optimized pegasos\n",
      "0.4425759754991532\n",
      "bad : 1.1459027315123274\n",
      "movie : 0.9793471019320472\n",
      "only : 0.7994670219853439\n",
      "have : 0.6928714190539643\n",
      "? : 0.6862091938707535\n",
      "no : 0.6795469686875426\n",
      "at : 0.6195869420386413\n",
      "they : 0.6062624916722186\n",
      "even : 0.5929380413057965\n",
      "could : 0.5263157894736851\n",
      "to : 0.4930046635576284\n"
     ]
    }
   ],
   "source": [
    "# 6.3 Optimized pagasos implementation\n",
    "def pegasos_optimized(Lambda, data, max_iters, print_kinks = False):\n",
    "    '''\n",
    "    Args\n",
    "        Lambda - Regularization parameter\n",
    "    Returns:\n",
    "        w - a asparse weight vector w\n",
    "    '''\n",
    "    iters = 0\n",
    "    t=1\n",
    "    w=Counter()\n",
    "    s=1\n",
    "    kinks=0\n",
    "    # a simple termination condition for now\n",
    "    while(iters<max_iters):\n",
    "        iters = iters+1\n",
    "        for data in train_data:\n",
    "            t=t+1    \n",
    "            eta = 1.0/(t*Lambda)\n",
    "            y = data[1]\n",
    "            x = data[0]\n",
    "            # if s==0 reset the parameter\n",
    "            if (s==0):\n",
    "                s=1\n",
    "                w=Counter()\n",
    "            if print_kinks and dotProduct(x,w)==0:\n",
    "                print('Feature Vector:')\n",
    "                print(x)\n",
    "                print(\"Parameter Vector:\")\n",
    "                print(w)\n",
    "                kinks=kinks+1 \n",
    "            if(y*dotProduct(x,w)<1/s):\n",
    "                s = (1-eta*Lambda)*s\n",
    "                increment(w, ((1/s)*eta*y) ,x)\n",
    "            else: s = (1-eta*Lambda)*s\n",
    "    if print_kinks: print(\"Number of kinks in the plot {0}\".format(kinks))\n",
    "    k = Counter()\n",
    "    increment(k, s, w)\n",
    "    return k\n",
    "print(\"Time for optimized pegasos\")\n",
    "start = default_timer()\n",
    "w_opt2 = pegasos_optimized(0.1, train_data, 1)\n",
    "print(default_timer()-start)\n",
    "i =0\n",
    "for k,v in w_opt2.most_common():\n",
    "    print(k,\":\", v)\n",
    "    i=i+1\n",
    "    if (i>10): break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with normal pegasos\n",
      "34.0\n",
      "Loss with optimized pegasos\n",
      "34.0\n"
     ]
    }
   ],
   "source": [
    "# 6.5 0-1 loss computation\n",
    "def compute_loss(w, data):\n",
    "    '''\n",
    "    Return:\n",
    "    percent error using 0-1 loss\n",
    "    '''\n",
    "    loss = 0\n",
    "    for data_point in data:\n",
    "        x = data_point[0]\n",
    "        y = data_point[1]\n",
    "        if np.sign(dotProduct(x,w)) != np.sign(y): loss=loss+1\n",
    "    return (loss/len(data))*100\n",
    "\n",
    "#compute loss on validation dataset\n",
    "print(\"Loss with normal pegasos\")\n",
    "loss1 = compute_loss(w_opt1,validation_dataset)\n",
    "loss2 = compute_loss(w_opt2, validation_dataset)\n",
    "print(loss1)\n",
    "print(\"Loss with optimized pegasos\")\n",
    "print(loss2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running\n",
      "Lambda is 1e-06, loss is 19.0\n",
      "Lambda is 1e-05, loss is 19.0\n",
      "Lambda is 0.0001, loss is 20.4\n",
      "Lambda is 0.001, loss is 17.2\n",
      "Lambda is 0.01, loss is 18.0\n",
      "Lambda is 0.1, loss is 17.2\n",
      "Lambda is 1.0, loss is 20.599999999999998\n",
      "Lambda is 10.0, loss is 35.8\n",
      "Lambda is 100.0, loss is 50.2\n",
      "Lambda is 1000.0, loss is 50.0\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "# 6.6 Search for optimal Lambda\n",
    "# Write a module for searching best Lambda\n",
    "def get_opt_lambda(data, lambda_range):\n",
    "    '''\n",
    "    Search for an optimal lambda.\n",
    "    '''\n",
    "    loss_hist = list()\n",
    "    loss_min = np.inf\n",
    "    lambda_opt = None\n",
    "    # for a range of lambda calculate the loss and keep track of the minimum\n",
    "    for Lambda in lambda_range:\n",
    "        w = pegasos_optimized(Lambda, data, 30)\n",
    "        loss = compute_loss(w, validation_dataset)\n",
    "        loss_hist.append(loss)\n",
    "        print(\"Lambda is {0}, loss is {1}\".format(Lambda,loss))\n",
    "        # update the minimum\n",
    "        if loss<loss_min:\n",
    "            lambda_opt = Lambda\n",
    "            loss_min = loss\n",
    "            w_opt = w\n",
    "    return lambda_opt, loss_hist, w_opt\n",
    "\n",
    "# snippet to test lambda search\n",
    "print(\"Running\")\n",
    "Lambdas=list(map(lambda x:10**x,np.linspace(-6,3,10)))\n",
    "lambda_opt, loss_hist, w_opt = get_opt_lambda(train_data, Lambdas)\n",
    "print(lambda_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss against lambda\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(Lambdas,loss_hist)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.xscale('log')\n",
    "plt.title('Loss vs. Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Zoomed in Lambda search\n",
    "Lambdas=list(map(lambda x:10**x,np.linspace(-4,0,10)))\n",
    "lambda_opt, loss_hist,w_opt = get_opt_lambda(train_data, Lambdas)\n",
    "print(lambda_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Lambdas,loss_hist)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Loss')\n",
    "plt.xscale('log')\n",
    "plt.title('Loss vs. Lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 6.7 Plotting score against pecent error \n",
    "import pandas as pd\n",
    "validation_scores = list()\n",
    "error = list()\n",
    "for data in validation_dataset:\n",
    "    x= data[0]\n",
    "    y= data[1]\n",
    "    score = dotProduct(x,w_opt)\n",
    "    validation_scores.append(score)\n",
    "    error.append(np.sign(y)!=np.sign(score))\n",
    "data = pd.DataFrame(validation_scores)\n",
    "data['error'] = error\n",
    "data.columns = ['score', 'error']\n",
    "groups = data.groupby(pd.cut(data.score,10))\n",
    "groups.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear from above, the error is highest in the range with smallest absolute values of scores. Hence, higher the magnitude of score, higher are the chances of the prediction being correct; We can therefore think of magnitude of score as the confidence of our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#6.8 Investigating the kinks\n",
    "# On Training Data\n",
    "w_temp = pegasos_optimized(lambda_opt, train_data, 31, print_kinks=True)\n",
    "\n",
    "# On validation dataset\n",
    "validation_kinks = 0\n",
    "for data in validation_dataset:\n",
    "    x= data[0]\n",
    "    y= data[1]\n",
    "    if (dotProduct(x,w_temp)==0): validation_kinks+=1\n",
    "print(validation_kinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "1. There was only one point during the gradient descent when the dot product was zero\n",
    "2. That was in begining when the feature vector was empty.\n",
    "3. At any other point in the algorithm its highly improbable to have a zero dot product.\n",
    "4. Hence, we can skip update when the product goes to zero only if we can find a different way to initialize the parameter vector(other then empty initialization, may be randomly), not otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7. Detailed analysis can be found below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# List of wrongly predicted reviews\n",
    "wrong_predictions = list()\n",
    "for data in validation_dataset:\n",
    "    x=data[0]\n",
    "    y=data[1]\n",
    "    if (y*dotProduct(x,w_opt)<0):\n",
    "        wrong_predictions.append((x,y,dotProduct(x,w_opt)))\n",
    "i=0\n",
    "for review in wrong_predictions[:5]:\n",
    "    features = review[0]  # a dict\n",
    "    target = review[1]\n",
    "    score = review[2]\n",
    "    # get sorted w_i*x_ifor this review\n",
    "    contributions = Counter()\n",
    "    print(\"Wrong prediction {0} has true_value {1} and score of {2}\".format(i,target,score))\n",
    "    print(\"Following are the 10 greatest contributors to the predicted value\")\n",
    "    for word,count in features.most_common():\n",
    "        contributions[word] = abs(count*w_opt[word])\n",
    "    for word,contribution in contributions.most_common(10):\n",
    "        print(word, \"   \", contribution, \"   \", features[word], \"   \", w_opt[word])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As is evident from the results above, the first file has huge count of the word \"the\" and \"and\"; while the second file has high counts for the words 'and', 'the', 'as' etc; Words like these do not usually indicate anything about sentiment but contribute largely in our prediction model due to their high count in the review text.\n",
    "Removing these words from our dataset can help improve the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download stopwords from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_words = set(stopwords.words('english'))\n",
    "def remove_s_words(bw):\n",
    "    '''\n",
    "    Accepts a bag of words representation of input\n",
    "    Returns a bag of words with stop words removed.\n",
    "    '''\n",
    "    s_words = set(stopwords.words('english'))\n",
    "    for key,value in bw.most_common():\n",
    "        if key in s_words:\n",
    "            del bw[key]\n",
    "    return bw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a filtered dataset and use it for training model and test for improvement in accuracy\n",
    "def new_pegasos_optimized(Lambda, data, max_iters, remove_stopwords=False):\n",
    "    '''\n",
    "    Args\n",
    "        Lambda - Regularization parameter\n",
    "    Returns:\n",
    "        w - a asparse weight vector w\n",
    "    '''\n",
    "    iters = 0\n",
    "    t=1\n",
    "    w=Counter()\n",
    "    s=1\n",
    "    # a simple termination condition for now\n",
    "    while(iters<max_iters):\n",
    "        iters = iters+1\n",
    "        for data in train_data:\n",
    "            t=t+1    \n",
    "            eta = 1.0/(t*Lambda)\n",
    "            y = data[1]\n",
    "            x = data[0]\n",
    "            if remove_stopwords:\n",
    "                x = remove_s_words(x)\n",
    "            # if s==0 reset the parameter\n",
    "            if (s==0):\n",
    "                s=1\n",
    "                w=Counter()\n",
    "            if(y*dotProduct(x,w)<1/s):\n",
    "                s = (1-eta*Lambda)*s\n",
    "                increment(w, ((1/s)*eta*y) ,x)\n",
    "            else: s = (1-eta*Lambda)*s\n",
    "    k = Counter()\n",
    "    increment(k, s, w)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get optimal predictor\n",
    "w_opt_5 = new_pegasos_optimized(lambda_opt, train_data, 31)\n",
    "print(\"Loss with no optimization\")\n",
    "print(compute_loss(w_opt_5, validation_dataset))\n",
    "w_opt_6 = new_pegasos_optimized(lambda_opt, train_data, 31, remove_stopwords=True)\n",
    "print(\"Loss with stop words removed\")\n",
    "print(compute_loss(w_opt_6, validation_dataset))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As indicated in the analysis above, I removed the words which do not contribute towards sentiment, but get weighted higly due to high count. Addition of this feature lead to a 6% improvement in the error as can be seen above. \n",
    "Standard Error without the feature - \n",
    "percent_error - 24.8\n",
    "numer of wrong classification = 124\n",
    "p = 0.248\n",
    "standard error = 0.0193130008\n",
    "Standard Error with the feature :\n",
    "percent_error - 18.8\n",
    "numer of wrong classification = 94\n",
    "p = 0.188\n",
    "standard error = 0.01747317944\n",
    "\n",
    "Improvement in standard error = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Features to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding a n-grams to our featureset\n",
    "def add_ngram(review):\n",
    "    '''\n",
    "    Accpets a review and adds all consequtive words as a feature\n",
    "    '''\n",
    "    prev = ' '\n",
    "    ngrams = list()\n",
    "    for word in review[:-1]:\n",
    "        ngram = prev + \" \" + word\n",
    "        ngrams.append(ngram)\n",
    "        pre = word\n",
    "    return ngrams + review \n",
    "\n",
    "def not_features(review):\n",
    "    '''\n",
    "    Accpets a review and combines not and following word to form a \n",
    "    feature that is added to original list of features\n",
    "    '''\n",
    "    prev = ' '\n",
    "    ngrams = list()\n",
    "    for word in review[:-1]:\n",
    "        if prev == 'not':\n",
    "            ngram = prev + \" \" + word\n",
    "            ngrams.append(ngram)\n",
    "            pre = word\n",
    "    return ngrams + review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create updated datasets\n",
    "reviews\n",
    "updated_reviews = list(map(lambda x:add_ngram(x), reviews))\n",
    "updated_reviews2 = list(map(lambda x:not_features(x), updated_reviews))\n",
    "# split into train and test set\n",
    "updated_train = bag_of_words(updated_reviews2[:1500])\n",
    "updated_validation = bag_of_words(updated_reviews2[1500:2000])\n",
    "\n",
    "# \n",
    "w_opt_7 = new_pegasos_optimized(lambda_opt, updated_train, 31, remove_stopwords=True)\n",
    "print(len(w_opt_7))\n",
    "print(compute_loss(w_opt_7, updated_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is slight improvement with additione of new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
